---
layout: post
title:  "Confidence intervals & Unaccounted variance"
date:   2020-08-05
categories: update
---

In the past few years, I’ve been studying statistical and causal inference while working as a data scientist. I realize that most people are taught frequentist statistics to perform statistical inferences. But over time, I find the interpretation of p-values and confidence intervals unintuitive and, most of the time, renders them useless or impractical. In this article, I select one scenario that can result in meaningless confidence intervals and p-values to demonstrate my point.

In this article, I will work with the Fisher variant of confidence intervals and p-values. In this flavor of frequentist, if you repeatedly sample from the same population and calculate 95% confidence intervals, approximately 95% of all confidence intervals will contain the true parameter value. Let’s say that we tracked exam scores of 100 students over time. Each time before an exam, students either take a tutoring class to boost their exam scores or they don't. We want to know whether the tutoring class is effective or not. Assuming that exams are conditionally independent of each other given the student, we will test the effect of the tutoring class by conducting a Welch’s t-test between exam scores when students take a tutoring class and exam scores when students don't take a tutoring class (Note that fitting a linear regression model with a variable indicating whether the student takes a tutoring class or not as a predictor variable and exam scores as the dependent variable will return the same result).
Suppose that we know the true data generating process, and it turns out that the tutoring class affected the students’ exam scores differently for each of the students. Some students’ scores increased with the tutoring class. Some students’ scores decreased. And some students’ scores remained approximately the same. But overall, the average effect is 0. The effect of the tutoring class on students is drawn from a gaussian distribution: t ~ Gaussian(0, 5).


<p align="center">
<img src="{{ site.baseurl }}/images/cf_mlm/add_var_n20.png" width="30%">
<img src="{{ site.baseurl }}/images/cf_mlm/add_var_n50.png" width="30%"><br>
figure 1. 100 confidence intervals from simulated data with a) 20 data points per student, b) 50 data points per student, and c) 100 data points per student
</p>

I simulated 100 studies and calculated their respective 95% confidence intervals for different number of data points per student (20, 50, and 100 from left to right figure). The results are shown in the figures above.
According to the definition of confidence intervals, roughly 95 intervals should contain the true parameter, which is 0. However, most of the calculated confidence intervals don’t contain 0 at all (false statistical significance). Moreover, with more number of data points per student, there is less number of confidence intervals that actually contain 0. This happens because we ignore the variation in the effect of the tutoring class on students resulting in more overconfidence intervals with larger sample sizes. Let’s try removing the variation between student effect and redo the simulation again.

figure 2. 100 confidence intervals from a t-test of a non-varying effect between student data model

Now, most of the confidence intervals contain the true parameter value even with the t-test. But what if there’s a real variation between student effect, and we want to account for it? A method to handle additional variance is to use multilevel/hierarchical/mixed effect model. This family of models will allow you to specify an additional level of data that causes a variation in the effect. In this example, that is the student-level variation. This model will also allow us to estimate effects for each level unit separately. Meaning, in this case, we can obtain the effect of the tutoring class for each individual student as well, but we will ignore them for now.

figure 3. 100 confidence intervals from an estimate in a mixed effect model when there’s a variation in tutoring effect between students

The above figure shows 95% confidence intervals of the tutoring effect when we use a mixed effect model instead of a t-test (or OLS). The confidence intervals become much wider (not overconfident) and contain the true parameter value (0) roughly 95% of the time now.
This is just one of many other scenarios that can throw off your confidence intervals and p-values. In reality, things are even more complicated than the example I used here. And anything can mess with your confidence intervals. I may write about them later when I feel motivated :)
One thing you should be aware of is that statistical inference with mixed effect model can be tricky and requires a specific algorithm to calculate confidence intervals and p-values. In the frequentist paradigm, once you move beyond traditional statistical tests, inferences (or the quantification of uncertainty) become complicated. Another thing is when you use the mixed effect model, the average effect can become a little less relevant. Do you still care about the aggregate effect if you can estimate the more granular level of effects now? The answer is dependent on context and theory that drives your research question.
P.S.1 This article is also inspired by a preprint “generalizibility crisis” by Tal Yarkoni, in which he discusses issues that arise with mismatches between statistical models and verbal hypotheses more extensively (https://doi.org/10.31234/osf.io/jqw35).
P.S.2 Here’s the link to an R notebook that I used to produce analyses and figures for this article: https://htmlpreview.github.io/?https://raw.githubusercontent.com/chasnew/data_simulation_repo/master/unacc_var_mixedlm.html
